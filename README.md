### 项目说明

该脚本仅用于辅助漏洞大模型项目的手工审查任务，流程完全自动化

#### 文件结构

`DeepSeek` 文件夹内仅包含 `API_process.py` 和 `config.json` 两个文件，前者为程序启动脚本，后者为配置文件。

#### 配置文件说明

```bash
# config.json文件结构
{
    "INPUT_FILE": "your csv file path",
    "DEEPSEEK_API_KEY": "your_deepseek_api_key",
    "BAILIAN_API_KEY": "your_bailian_api_key",
    "SILICONFLOW_API_KEY": "your_siliconflow_api_key",
    "MAX_WORKERS": 24
}
```

- `INPUT_FILE`：你需要处理的 csv 文件路径
- `DEEPSEEK_API_KEY`：`R1` 模型的 `API-KEY`，程序默认使用官网提供的 API 服务，因此想要开箱即用该字段必须配置
- `MAX_WORKERS`：程序并发数，用于提高处理速度。由于 `DeepSeek` 不限制用户并发度，所以该字段理论上可以随便设，不过不建议设得太大（防止请求堵塞）。实测并发度为 20 的情况下处理 200 条数据大概需要 1 小时，你可以根据自身需要适当进行调整。
- 其余两个参数分别表示百炼和硅基流动的 `API_KEY`，如果想使用这两个厂商的 API，请修改 `API_process.py` 583 行。

#### 程序运行

```bash
# 该脚本的环境依赖很少，按照编辑器报错简单配一下就行
python API_process.py
```

> 为了防止连接中断，强烈建议把程序挂后台运行

#### 输出格式说明

程序默认输出文件名为 `output_with_analysis.csv`，输出文件在原始文件基础上增加了六个新的字段，各字段说明如下：

- `ds_think`：模型的完整思维链内容
- `ds_output`：模型的漏洞检测结果
- `ds_result`：模型检测出的漏洞编号（也可能检测出无漏洞）
- `is_correct`：模型检测出的漏洞编号是否覆盖所有的真实标签
- `correct_output`：我们期待的模型正确输出
- `ds_think_reduced`：精简过后的思维链

输出文件格式跟要求提交的格式稍微有点出入，因此需要手工修改一下。

这里的 `ds_think_reduced` 是精简过后的思维链，也就是老师要求的 `ds_think`，因此提交之前需要把 `ds_think` 字段的内容替换成 `ds_think_reduced` 字段，然后 `is_correct` 字段需要删掉，最终只留下 `替换后的 ds_think` + `ds_output` +`ds_result` + `correct_output` 四个字段。

> 之所以输出多余的字段是考虑到方便后续的人工二次审查，但是这玩意实在太多了，我是顶不住了，就只简单检查了下。如果你想自己审一下可以对照着这些字段自己检查校正。

不过需要注意的是 `is_correct` 字段需要看一下。如果该字段输出无漏洞，不一定是模型没检测到，可能只是模型没输出有效格式的 CWE 编号导致正则表达式匹配失败。如果出现这种情况则需要我们手工检查一下。

#### 注意事项（一定要看！！！）

- 该脚本中存在两处重试机制（精简思维链+正确答案生成），因此处理一条数据最多可能调用 6 次 API。由于使用的是 `R1`，所以 token 消耗较大。实测处理 470 条左右数据需要花费 60+，我自己处理 200 条数据花了大概 30 元，因此强烈推荐 `00:30` 之后使用，25 折还是很实惠的
- 虽然程序兼容 excel 和 csv 处理，但是还是推荐使用 csv 格式作为输入。（excel 处理部分代码优化不足，因此使用 xlsx 文件的话可能会遇到如输出截断等未知错误）
- 正式运行程序之前一定要检查一下你的 csv 文件格式是否正确，确保 csv 每一行对应的是一条完整的数据。有些行的数据转为 csv 格式后一行会变成两行，这种数据务必手工校正一下，否则程序可能会中途报错导致浪费大量 token
- 最后再次强调，程序记得挂后台，防止连接中断浪费 token